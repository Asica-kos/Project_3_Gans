# **Project 3 - ETL Pipeline Project: Gans**

Welcome to the **Gans** project! I was hired again by the imaginary company called **Gans** which is a startup developing an e-scooter-sharing system, and dreaming of increased usage of e-scooters in big cities.
However, this time it was different. There was no ready-to-use database, but I would have to create one from scratch. 
This project involves developing a complete **ETL (Extract, Transform, Load) pipeline**, which extracts data from multiple sources, transforms it using Python and Pandas, and loads it into a SQL database for organized storage and analysis.

## Key Features

1. **Data extraction**
   
      **Web Scraping**: to help the company, we first need to have a closer look at the most populated cities in Germany (in my case I focused analysis on Berlin, Hamburg and Munich)

      **APIs** : APIs provide a structured and standardized means of accessing and retrieving data. In this project I used weather and flight APIs
   
3. **Data transformation**: clean and process the extracted data using Python and Pandas
   
4. **Data storage**: loading the clean, transformed data into a SQL database, where it is stored in organized tables


## What I've learnt?

1. **Building an End-to-End ETL Pipeline**: learn how to design and implement a full ETL (Extract, Transform, Load) pipeline that automates data collection, transformation, and storage
   
3. **Working with APIs**: gaining experience in accessing and retrieving data from APIs
   
5. **Data Cleaning and Transformation with Pandas**: strengthening my skills in data manipulation using Python and Pandas
   
7. **SQL Database Management**: mastering the fundamentals of SQL databases, including how to design database schemas, create tables, and insert data efficiently
   
9. **Automation and Workflow Optimization**: discovering how ETL pipelines benefit from automated data processing and storage

